# optimizers:
# "adam": [lr, beta1=0.9, beta2=0.99, weight_decay=0, amsgrad=False] https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam
# "adamw": [lr, beta1=0.9, beta2=0.99, weight_decay=0, amsgrad=False] https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW
# "sgd": [lr, momentum=0, weight_decay=0, nesterov=False, dampening=0] https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD  

# schedulers
# "LinearLR": [end_factor] https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR
# "ReduceLROnPlateau": [factor=0.1, patience=10, threshold=1e-4, threshold_mode="rel", cooldown=0, eps=1e-8] https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau
# "MultiStepLR": [gamma, milestones] https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR
# "ExponentialLR": [gamma] https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR

# transforms
# -

#=| Main |================================================================================#
# [train, inference]
mode: train

name: uppicres

# [seg, clr]
task: seg

# [SegUnet, ]
nn_model: SegUnet

# [cpu, cuda|cuda:0]
device: cuda

# [0-disable, 256, 512, 1024]
crop: 256

epoch: 100

# [0-disable, n]
transform_data_every_n_epoch: 50

logs_path: logs/

telegram_send_logs: True



#=| Data |================================================================================#
datasets:
  seg_dataset:

    #Dataloader params
    dataloader:
      train:
        batch_size: 4
        shuffle: false
        # num_workers: 2 #TODO Must be
        pin_memory: false
        drop_last: false

      test:
        batch_size: 4
        shuffle: false
        drop_last: false

    train:
      imgs_path: "data/seg_damage_data/train/images/" 
      masks_path: "data/seg_damage_data/train/masks/"

      # [true, false]
      load_to_ram: true

      # [GrayScale, ColorJitter, RandomHorizontalFlip, RandomPosterize, 
      # RandomAdjustSharpness, RandomVerticalFlip, RandomEqualize]
      transforms: 
        colorjitter:
          p: .6
          brightness: [.1, .2, .3, .4]
          contrast: [.1, .2, .3, .4]
          saturation: [.1, .2, .3, .4]
          hue: [.1, .2, .3, .4]

        randomposterize:
          p: .3
          bits: [4, 6]
        
        randomadjustsharpness:
          p: .3
          sharpness_factor: [4, 6, 8, 10]

        randomequalize:
          p: .3

        grayscale:
          p: .2
          num_output_channels: 3

        randomhorizontalflip:
          p: .3

        randomverticalflip:
          p: .3

    test:
      imgs_path: "data/seg_damage_data/test/images/"
      masks_path: "data/seg_damage_data/test/masks/"

      # [ture, false]
      load_to_ram: true
  
  clear_damage_dataset:
    train: "some path"
    test: "some path"


#=| NN's |================================================================================#
nns:
  seg:
    # [bacc, broc-auc, dice, None]
    metrics: [bacc, broc-auc, dice] 

    # [bce]
    criterion:
      bce: 
        pos_weight: [2]

    models:
      SegUnet:
        in_channels: 3 # DO NOT CHANGE!
        out_channels: 1 # DO NOT CHANGE!
        activation_function: "gelu"  # [relu, gelu]  #TODO Add LeakyReLu

        optimizer: 
          name: adam
          lr: 1e-3
        
        scheduler:
          scheme: ReduceLROnPlateau
          lr: 1e-3
          factor: .1
          patience: 10
          threshold: 1e-3
          eps: 1e-9
      
  