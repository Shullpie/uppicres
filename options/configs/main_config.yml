# optimizers:
# "adam": [lr, beta1=0.9, beta2=0.99, weight_decay=0, amsgrad=False] https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam
# "adamw": [lr, beta1=0.9, beta2=0.99, weight_decay=0, amsgrad=False] https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW
# "sgd": [lr, momentum=0, weight_decay=0, nesterov=False, dampening=0] https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD  

# schedulers
# "LinearLR": [end_factor] https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR
# "ReduceLROnPlateau": [factor=0.1, patience=10, threshold=1e-4, threshold_mode="rel", cooldown=0, eps=1e-8] https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau
# "MultiStepLR": [gamma, milestones] https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR
# "ExponentialLR": [gamma] https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR

# transforms
# -

#=| Main |================================================================================#
# [train, inference]
mode: train

name: uppicres

# [seg, clr]
task: seg

# [SegUnet, ]
nn_model: SegUnet

# [cpu, cuda|cuda:0]
device: cuda

# [0-disable, 256, 512, 1024]
crop: 256

epoch: 100

# [0-disable, n]
transform_data_every_n_epoch: 2
make_checkpoint_every_n_epoch: 2

logs_path: logs/

telegram_send_logs: True



#=| Data |================================================================================#
datasets:
  seg_dataset:
    #Dataloader params
    dataloader:
      train:
        batch_size: 3
        shuffle: true
        num_workers: 2
        pin_memory: true
        drop_last: false

      test:
        batch_size: 3
        shuffle: false
        drop_last: false

    train:
      imgs_path: "data/seg_damage_data/train/images/" 
      masks_path: "data/seg_damage_data/train/masks/"

      # [true, false]
      load_to_ram: true

      #mean and std of all images in dataset
      normalize:
        mean: [0.4941, 0.4605, 0.4085]
        std: [0.1954, 0.1875, 0.1738]
        
      # [GrayScale, ColorJitter, RandomHorizontalFlip, RandomPosterize, 
      # RandomAdjustSharpness, RandomVerticalFlip, RandomEqualize]
      transforms:
        colorjitter:
          p: .2
          brightness: [.1, .2, .3, .4]
          contrast: [.1, .2, .3, .4]
          saturation: [.1, .2, .3, .4]
          hue: [.1, .2, .3]

        randomposterize:
          p: 0.1
          bits: [4, 6]
        
        randomadjustsharpness:
          p: .2
          sharpness_factor: [4, 6, 8]

        randomequalize:
          p: .2

        grayscale:
          p: .1
          num_output_channels: 3

        randomhorizontalflip:
          p: .3

        randomverticalflip:
          p: .3

    test:
      imgs_path: "data/seg_damage_data/test/images/"
      masks_path: "data/seg_damage_data/test/masks/"

      # [ture, false]
      load_to_ram: true
  
  clear_damage_dataset:
    train: "some path"
    test: "some path"


#=| NN's |================================================================================#
nns:
  seg:
    # [bacc, broc-auc, dice, None]
    metrics: [bacc, broc-auc, dice] 

    # [bce]
    criterion:
      bce: 
        pos_weight: [10.27]

    models:
      SegUnet:
        in_channels: 3 # DO NOT CHANGE!
        out_channels: 1 # DO NOT CHANGE!
        activation_function: "relu"  # [relu, gelu]  #TODO Add LeakyReLu

        optimizer: 
          name: adam
          lr: 1e-3
        
        scheduler:
          scheme: ReduceLROnPlateau
          factor: .2
          patience: 5
          threshold: 1e-4
          eps: 1e-9
      
  