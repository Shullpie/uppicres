# optimizers:
# "adam": [lr, beta1=0.9, beta2=0.99, weight_decay=0, amsgrad=False] https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam
# "adamw": [lr, beta1=0.9, beta2=0.99, weight_decay=0, amsgrad=False] https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW
# "sgd": [lr, momentum=0, weight_decay=0, nesterov=False, dampening=0] https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD

# schedulers
# "LinearLR": [end_factor] https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html#torch.optim.lr_scheduler.LinearLR
# "ReduceLROnPlateau": [factor=0.1, patience=10, threshold=1e-4, threshold_mode="rel", cooldown=0, eps=1e-8] https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau
# "MultiStepLR": [gamma, milestones] https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR
# "ExponentialLR": [gamma] https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR

# transforms
# -
# TODO Refactore this file
#=| Main |================================================================================#
# [train, inference]
mode: train

name: uppicres

# [seg, clr]
task: clr

# [SegUnet, ]
nn_model: SegUnetWide

# [0, path]
checkpoint: logs\checkpoints\segunetwide_adamax_reducelronplateau_crop256\20_epoch.pt

# [cpu, cuda|cuda:0]
device: cuda

# [0-disable, 256, 512, 1024]
crop: 256

epoch: 1000

# [0-disable, n]
transform_data_every_n_epoch: 5
make_checkpoint_every_n_epoch: 5

logs_path: logs/

telegram_send_logs: True


#=| Data |================================================================================#
datasets:
  seg_dataset:
    #Dataloader params
    dataloader:
      train:
        batch_size: 1
        shuffle: true
        num_workers: 2
        pin_memory: true
        drop_last: false

      test:
        batch_size: 1
        shuffle: false
        drop_last: false

    train:
      imgs_path: "data/seg_damage_data/train/images/"
      masks_path: "data/seg_damage_data/train/masks/"

      # [true, false]
      load_to_ram: true

      #mean and std of all images in dataset
      normalize:
        mean: [0.4941, 0.4605, 0.4085]
        std: [0.1446, 0.1396, 0.1307]

      # [GrayScale, ColorJitter, RandomHorizontalFlip, RandomPosterize,
      # RandomAdjustSharpness, RandomVerticalFlip, RandomEqualize]
      transforms:
        colorjitter:
          p: .2
          brightness: [.1, .2, .3, .4, .5]
          contrast: [.1, .2, .3, .4, .5]
          saturation: [.1, .2, .3, .4, .5]
          hue: [.1, .2, .3]

        randomposterize:
          p: .1
          bits: [4, 6]

        randomadjustsharpness:
          p: .2
          sharpness_factor: [4, 6, 8]

        randomequalize:
          p: .2

        grayscale:
          p: .1
          num_output_channels: 3

        randomhorizontalflip:
          p: .5

        randomverticalflip:
          p: .5

    test:
      imgs_path: "data/seg_damage_data/test/images/"
      masks_path: "data/seg_damage_data/test/masks/"

      # [true, false]
      load_to_ram: true

      normalize:
        mean: [0.4941, 0.4605, 0.4085]
        std: [0.1954, 0.1875, 0.1738]



  clr_dataset:
    #Dataloader params
    dataloader:
      train:
        batch_size: 10
        shuffle: false
        num_workers: 2
        pin_memory: true
        drop_last: false

      test:
        batch_size: 10
        shuffle: false
        drop_last: false

    train:
      imgs_path: "data/clr_damage_data/train/images/"
      masks_path: "data/clr_damage_data/train/masks/"

      # [true, false]
      load_to_ram: false

      normalize:
        mean: [0.5141, 0.4494, 0.4155]
        std: [0.2474, 0.2375, 0.2348]

      # [GrayScale, ColorJitter, RandomHorizontalFlip, RandomPosterize,
      # RandomAdjustSharpness, RandomVerticalFlip, RandomEqualize]
      transforms:
        colorjitter:
          p: .2
          brightness: [.1, .2, .3, .4, .5]
          contrast: [.1, .2, .3, .4, .5]
          saturation: [.1, .2, .3, .4, .5]
          hue: [.1, .2, .3]

        randomposterize:
          p: .1
          bits: [4, 6]

        randomadjustsharpness:
          p: .2
          sharpness_factor: [4, 6, 8]

        randomequalize:
          p: .2

        grayscale:
          p: .1
          num_output_channels: 3

        randomhorizontalflip:
          p: .5

        randomverticalflip:
          p: .5

    test:
      imgs_path: "data/clr_damage_data/test/images/"
      masks_path: "data/clr_damage_data/test/masks/"

      # [true, false]
      load_to_ram: false

      normalize:
        mean: [0.5141, 0.4494, 0.4155]
        std: [0.2474, 0.2375, 0.2348]


#=| NN's |================================================================================#
nns:
  seg:
    # [bacc, broc-auc, dice, None]
    metrics: [bacc, broc-auc, dice]

    # [bce]
    criterion:
      bce:
        pos_weight: [10.27]

    models:
      Unet:
        in_channels: 3 # DO NOT CHANGE!
        out_channels: 1 # DO NOT CHANGE!
        activation_function: "relu"  # [relu, gelu]  #TODO Add LeakyReLu

        optimizer:
          name: adamax
          lr: 1e-3

        scheduler:
          scheme: ReduceLROnPlateau
          factor: .1
          patience: 15
          cooldown: 5
          threshold: 1e-6
          eps: 1e-12

      UnetWide:
        in_channels: 3 # DO NOT CHANGE!
        out_channels: 1 # DO NOT CHANGE!
        activation_function: "relu"

        optimizer:
          name: adamax
          lr: 1e-3

        scheduler:
          scheme: ReduceLROnPlateau
          factor: .1
          patience: 15
          cooldown: 10
          threshold: 1e-6
          eps: 1e-12

  clr:
  # [bacc, broc-auc, dice, None]
  metrics: []

  # [bce]
  criterion:

  models:
    Unet:
      in_channels: 4 # DO NOT CHANGE!
      out_channels: 3 # DO NOT CHANGE!
      activation_function: "relu"  # [relu, gelu]  #TODO Add LeakyReLu

      optimizer:
        name: adamax
        lr: 1e-3

      scheduler:
        scheme: ReduceLROnPlateau
        factor: .1
        patience: 15
        cooldown: 5
        threshold: 1e-6
        eps: 1e-12

    UnetWide:
      in_channels: 3 # DO NOT CHANGE!
      out_channels: 1 # DO NOT CHANGE!
      activation_function: "relu"

      optimizer:
        name: adamax
        lr: 1e-3

      scheduler:
        scheme: ReduceLROnPlateau
        factor: .1
        patience: 15
        cooldown: 10
        threshold: 1e-6
        eps: 1e-12
